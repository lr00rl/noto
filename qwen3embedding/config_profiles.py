# config_profiles.py
"""
ä¸åŒæ˜¾å­˜å¤§å°çš„é…ç½®æ–‡ä»¶
è‡ªåŠ¨æ ¹æ®ç¡¬ä»¶é€‰æ‹©æœ€ä½³é…ç½®
"""

import torch
import psutil
from dataclasses import dataclass
from typing import Dict, Any, Optional
import json

@dataclass
class MemoryProfile:
    """å†…å­˜é…ç½®æ–‡ä»¶"""
    name: str
    model_name: str
    max_batch_size: int
    max_length: int
    precision: str
    gradient_checkpointing: bool
    cpu_offload: bool
    max_concurrent_requests: int
    cleanup_frequency: int  # æ¯Nä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡å†…å­˜
    description: str

class ProfileManager:
    """é…ç½®æ–‡ä»¶ç®¡ç†å™¨"""

    def __init__(self):
        self.profiles = {
            # ä½æ˜¾å­˜é…ç½® (2-4GB)
            "low_memory": MemoryProfile(
                name="low_memory",
                model_name="Qwen/Qwen3-Embedding-0.6B",
                max_batch_size=2,
                max_length=256,
                precision="fp16",
                gradient_checkpointing=True,
                cpu_offload=True,
                max_concurrent_requests=10,
                cleanup_frequency=1,  # æ¯ä¸ªæ‰¹æ¬¡éƒ½æ¸…ç†
                description="é€‚ç”¨äº2-4GBæ˜¾å­˜çš„ä½é…ç½®"
            ),

            # ä¸­ä½æ˜¾å­˜é…ç½® (4-6GB)
            "medium_low": MemoryProfile(
                name="medium_low",
                model_name="Qwen/Qwen3-Embedding-0.6B",
                max_batch_size=4,
                max_length=512,
                precision="fp16",
                gradient_checkpointing=True,
                cpu_offload=False,
                max_concurrent_requests=20,
                cleanup_frequency=2,
                description="é€‚ç”¨äº4-6GBæ˜¾å­˜"
            ),

            # ä¸­ç­‰æ˜¾å­˜é…ç½® (6-8GB)
            "medium": MemoryProfile(
                name="medium",
                model_name="Qwen/Qwen3-Embedding-4B",
                max_batch_size=8,
                max_length=512,
                precision="fp16",
                gradient_checkpointing=True,
                cpu_offload=False,
                max_concurrent_requests=50,
                cleanup_frequency=3,
                description="é€‚ç”¨äº6-8GBæ˜¾å­˜"
            ),

            # ä¸­é«˜æ˜¾å­˜é…ç½® (8-12GB)
            "medium_high": MemoryProfile(
                name="medium_high",
                model_name="Qwen/Qwen3-Embedding-4B",
                max_batch_size=16,
                max_length=1024,
                precision="fp16",
                gradient_checkpointing=False,
                cpu_offload=False,
                max_concurrent_requests=100,
                cleanup_frequency=5,
                description="é€‚ç”¨äº8-12GBæ˜¾å­˜"
            ),

            # é«˜æ˜¾å­˜é…ç½® (12GB+)
            "high_memory": MemoryProfile(
                name="high_memory",
                model_name="Qwen/Qwen3-Embedding-8B",
                max_batch_size=32,
                max_length=2048,
                precision="fp16",
                gradient_checkpointing=False,
                cpu_offload=False,
                max_concurrent_requests=200,
                cleanup_frequency=10,
                description="é€‚ç”¨äº12GB+æ˜¾å­˜"
            ),

            # CPUæ¨¡å¼
            "cpu_only": MemoryProfile(
                name="cpu_only",
                model_name="Qwen/Qwen3-Embedding-0.6B",
                max_batch_size=1,
                max_length=512,
                precision="fp32",
                gradient_checkpointing=False,
                cpu_offload=True,
                max_concurrent_requests=5,
                cleanup_frequency=1,
                description="çº¯CPUæ¨¡å¼"
            )
        }

    def detect_optimal_profile(self) -> MemoryProfile:
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®"""
        if not torch.cuda.is_available():
            print("ğŸ” æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return self.profiles["cpu_only"]

        # è·å–GPUä¿¡æ¯
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        system_memory_gb = psutil.virtual_memory().total / 1024**3

        print(f"ğŸ” ç¡¬ä»¶æ£€æµ‹:")
        print(f"   GPUæ˜¾å­˜: {gpu_memory_gb:.1f}GB")
        print(f"   ç³»ç»Ÿå†…å­˜: {system_memory_gb:.1f}GB")

        # æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©é…ç½®
        if gpu_memory_gb >= 12:
            profile = self.profiles["high_memory"]
        elif gpu_memory_gb >= 8:
            profile = self.profiles["medium_high"]
        elif gpu_memory_gb >= 6:
            profile = self.profiles["medium"]
        elif gpu_memory_gb >= 4:
            profile = self.profiles["medium_low"]
        else:
            profile = self.profiles["low_memory"]

        # å¦‚æœç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œé™çº§é…ç½®
        if system_memory_gb < 8:
            print("âš ï¸  ç³»ç»Ÿå†…å­˜ä¸è¶³8GBï¼Œé™çº§åˆ°ä½å†…å­˜é…ç½®")
            profile = self.profiles["low_memory"]

        print(f"ğŸ“‹ é€‰æ‹©é…ç½®: {profile.name} - {profile.description}")
        return profile

    def get_profile(self, name: str) -> Optional[MemoryProfile]:
        """è·å–æŒ‡å®šé…ç½®"""
        return self.profiles.get(name)

    def list_profiles(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®"""
        print("ğŸ“‹ å¯ç”¨é…ç½®:")
        for name, profile in self.profiles.items():
            print(f"   {name}: {profile.description}")
            print(f"     æ¨¡å‹: {profile.model_name}")
            print(f"     æ‰¹æ¬¡å¤§å°: {profile.max_batch_size}")
            print(f"     æœ€å¤§é•¿åº¦: {profile.max_length}")
            print()

    def save_config(self, profile: MemoryProfile, filepath: str = "current_config.json"):
        """ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = {
            "profile_name": profile.name,
            "model_name": profile.model_name,
            "max_batch_size": profile.max_batch_size,
            "max_length": profile.max_length,
            "precision": profile.precision,
            "gradient_checkpointing": profile.gradient_checkpointing,
            "cpu_offload": profile.cpu_offload,
            "max_concurrent_requests": profile.max_concurrent_requests,
            "cleanup_frequency": profile.cleanup_frequency,
            "description": profile.description
        }

        with open(filepath, 'w') as f:
            json.dump(config_dict, f, indent=2)
        print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ° {filepath}")

# åŠ¨æ€é…ç½®è°ƒæ•´å™¨
class DynamicConfigAdjuster:
    """åŠ¨æ€é…ç½®è°ƒæ•´å™¨ - è¿è¡Œæ—¶æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´å‚æ•°"""

    def __init__(self, initial_profile: MemoryProfile):
        self.profile = initial_profile
        self.adjustment_history = []
        self.oom_count = 0

    def handle_oom_error(self):
        """å¤„ç†æ˜¾å­˜ä¸è¶³é”™è¯¯"""
        self.oom_count += 1
        old_batch_size = self.profile.max_batch_size

        # å‡åŠæ‰¹æ¬¡å¤§å°
        new_batch_size = max(1, self.profile.max_batch_size // 2)
        self.profile.max_batch_size = new_batch_size

        # è®°å½•è°ƒæ•´
        adjustment = {
            "type": "oom_reduction",
            "old_batch_size": old_batch_size,
            "new_batch_size": new_batch_size,
            "oom_count": self.oom_count
        }
        self.adjustment_history.append(adjustment)

        print(f"âš ï¸  æ˜¾å­˜ä¸è¶³ï¼Œæ‰¹æ¬¡å¤§å°è°ƒæ•´: {old_batch_size} -> {new_batch_size}")

        return new_batch_size

    def optimize_batch_size(self, success_rate: float, avg_processing_time: float):
        """æ ¹æ®æˆåŠŸç‡å’Œå¤„ç†æ—¶é—´ä¼˜åŒ–æ‰¹æ¬¡å¤§å°"""
        if success_rate > 0.95 and avg_processing_time < 2.0:
            # æ€§èƒ½è‰¯å¥½ï¼Œå°è¯•å¢åŠ æ‰¹æ¬¡
            if self.profile.max_batch_size < 32:
                old_size = self.profile.max_batch_size
                self.profile.max_batch_size = min(32, int(old_size * 1.5))
                print(f"ğŸ“ˆ æ€§èƒ½è‰¯å¥½ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°: {old_size} -> {self.profile.max_batch_size}")

        elif success_rate < 0.8:
            # æˆåŠŸç‡ä½ï¼Œå‡å°‘æ‰¹æ¬¡
            old_size = self.profile.max_batch_size
            self.profile.max_batch_size = max(1, old_size - 1)
            print(f"ğŸ“‰ æˆåŠŸç‡è¾ƒä½ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°: {old_size} -> {self.profile.max_batch_size}")

    def get_current_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®"""
        return {
            "max_batch_size": self.profile.max_batch_size,
            "max_length": self.profile.max_length,
            "precision": self.profile.precision,
            "cleanup_frequency": self.profile.cleanup_frequency,
            "adjustments_made": len(self.adjustment_history),
            "oom_count": self.oom_count
        }

# æ™ºèƒ½èµ„æºç›‘æ§å™¨
class ResourceMonitor:
    """æ™ºèƒ½èµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.history = []
        self.alert_thresholds = {
            "gpu_memory": 90,  # GPUæ˜¾å­˜ä½¿ç”¨ç‡é˜ˆå€¼
            "system_memory": 85,  # ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
            "temperature": 85  # GPUæ¸©åº¦é˜ˆå€¼ï¼ˆå¦‚æœå¯è·å–ï¼‰
        }

    def get_current_usage(self) -> Dict[str, Any]:
        """è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""
        usage = {
            "timestamp": torch.cuda.Event().record() if torch.cuda.is_available() else None,
            "system_memory": psutil.virtual_memory().percent
        }

        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            usage.update({
                "gpu_memory_allocated": allocated,
                "gpu_memory_total": total,
                "gpu_memory_percent": (allocated / total) * 100,
                "gpu_memory_available": total - allocated
            })

        return usage

    def check_resource_pressure(self) -> Dict[str, bool]:
        """æ£€æŸ¥èµ„æºå‹åŠ›"""
        usage = self.get_current_usage()
        pressure = {}

        # ç³»ç»Ÿå†…å­˜å‹åŠ›
        pressure["high_system_memory"] = usage["system_memory"] > self.alert_thresholds["system_memory"]

        # GPUæ˜¾å­˜å‹åŠ›
        if torch.cuda.is_available():
            pressure["high_gpu_memory"] = usage["gpu_memory_percent"] > self.alert_thresholds["gpu_memory"]
            pressure["low_gpu_memory"] = usage["gpu_memory_available"] < 1.0  # å°‘äº1GBå¯ç”¨

        return pressure

    def suggest_adjustments(self) -> List[str]:
        """æ ¹æ®èµ„æºå‹åŠ›æä¾›è°ƒæ•´å»ºè®®"""
        pressure = self.check_resource_pressure()
        suggestions = []

        if pressure.get("high_gpu_memory", False):
            suggestions.append("é™ä½æ‰¹æ¬¡å¤§å°")
            suggestions.append("å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
            suggestions.append("è€ƒè™‘ä½¿ç”¨CPU offload")

        if pressure.get("high_system_memory", False):
            suggestions.append("å‡å°‘å¹¶å‘è¯·æ±‚æ•°")
            suggestions.append("å¢åŠ å†…å­˜æ¸…ç†é¢‘ç‡")

        if pressure.get("low_gpu_memory", False):
            suggestions.append("ç«‹å³æ‰§è¡Œå†…å­˜æ¸…ç†")
            suggestions.append("æš‚åœæ¥æ”¶æ–°è¯·æ±‚")

        return suggestions

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """é…ç½®ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹"""
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    manager = ProfileManager()

    # åˆ—å‡ºæ‰€æœ‰é…ç½®
    manager.list_profiles()

    # è‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®
    optimal_profile = manager.detect_optimal_profile()

    # ä¿å­˜é…ç½®
    manager.save_config(optimal_profile)

    # åˆ›å»ºåŠ¨æ€è°ƒæ•´å™¨
    adjuster = DynamicConfigAdjuster(optimal_profile)

    # åˆ›å»ºèµ„æºç›‘æ§å™¨
    monitor = ResourceMonitor()

    # æ£€æŸ¥å½“å‰èµ„æºçŠ¶æ€
    current_usage = monitor.get_current_usage()
    print("ğŸ“Š å½“å‰èµ„æºä½¿ç”¨:")
    for key, value in current_usage.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.2f}")
        else:
            print(f"   {key}: {value}")

    # æ£€æŸ¥èµ„æºå‹åŠ›
    pressure = monitor.check_resource_pressure()
    if any(pressure.values()):
        print("âš ï¸  æ£€æµ‹åˆ°èµ„æºå‹åŠ›:")
        for issue, present in pressure.items():
            if present:
                print(f"   {issue}")

        suggestions = monitor.suggest_adjustments()
        print("ğŸ’¡ å»ºè®®:")
        for suggestion in suggestions:
            print(f"   - {suggestion}")

if __name__ == "__main__":
    main()
